{
    "version": "https://jsonfeed.org/version/1",
    "title": "The Study",
    "description": "",
    "home_page_url": "https://chandownbytheriver.github.io",
    "feed_url": "https://chandownbytheriver.github.io/feed.json",
    "user_comment": "",
    "icon": "https://chandownbytheriver.github.io/media/website/Q3svdBhDQtubBB8M7CVI-1-1bsc1.jpg",
    "author": {
        "name": "Chandler Rottenberg"
    },
    "items": [
        {
            "id": "https://chandownbytheriver.github.io/using-noise-vectors-in-logistic-regression.html",
            "url": "https://chandownbytheriver.github.io/using-noise-vectors-in-logistic-regression.html",
            "title": "Using Noise Vectors in Logistic Regression",
            "summary": "Generally to perform a prediction with logistic regression, large datasets are used to train a model that can then look at a new case and predict how it will perform. What if we don't have a large dataset though? We might run into issues like&hellip;",
            "content_html": "<p>Generally to perform a prediction with logistic regression, large datasets are used to train a model that can then look at a new case and predict how it will perform. What if we don't have a large dataset though? We might run into issues like overfitting, or relationships between variables can't be fully captured. Either of these will result in poor performance when it comes time to make a prediction. Consider a variable that has an inverse correlation with your target variable, but the available data doesn't capture that. Your model may assume a positive relationship between the two, and upon being tested against unseen data it will produce inaccurate guesses.</p>\n<p>Is there a way that we could generate artifical data in a broad enough manner to both avoid overfitting and capture variable relationships?</p>",
            "author": {
                "name": "Chandler Rottenberg"
            },
            "tags": [
            ],
            "date_published": "2024-03-25T22:13:12-04:00",
            "date_modified": "2024-03-25T22:13:12-04:00"
        },
        {
            "id": "https://chandownbytheriver.github.io/creating-a-neural-network-to-determine-the-strength-of-concrete.html",
            "url": "https://chandownbytheriver.github.io/creating-a-neural-network-to-determine-the-strength-of-concrete.html",
            "title": "Determining the Effects of Different Parameters in Neural Network Creation",
            "summary": "While the architechture of a neural network can be easily accomplished with a few lines of code, building one from the ground up helps to ensure clarity and that each part works as it should. Further, it allows us to alter each part of the&hellip;",
            "content_html": "<p>     While the architechture of a neural network can be easily accomplished with a few lines of code, building one from the ground up helps to ensure clarity and that each part works as it should. Further, it allows us to alter each part of the network one by one to find the optimal parameters for different situations. How do different numbers of epochs increase or decrease accuracy?</p>\n<figure class=\"post__image\"><img loading=\"lazy\"  src=\"https://chandownbytheriver.github.io/media/posts/2/basiclibraries.PNG\" alt=\"\" width=\"421\" height=\"145\" sizes=\"100vw\" srcset=\"https://chandownbytheriver.github.io/media/posts/2/responsive/basiclibraries-xs.PNG 300w ,https://chandownbytheriver.github.io/media/posts/2/responsive/basiclibraries-sm.PNG 480w ,https://chandownbytheriver.github.io/media/posts/2/responsive/basiclibraries-md.PNG 768w ,https://chandownbytheriver.github.io/media/posts/2/responsive/basiclibraries-lg.PNG 1024w ,https://chandownbytheriver.github.io/media/posts/2/responsive/basiclibraries-xl.PNG 1360w ,https://chandownbytheriver.github.io/media/posts/2/responsive/basiclibraries-2xl.PNG 1600w\"></figure>\n<p>We'll start off by loading Keras for the model's formatting as well as pandas and SciKit Learn for statistics and equations.</p>\n<p>Next, we'll load up our data file and generate a description of the dataset. Further, we'll check for any missing values that might require cleaning.</p>\n<figure class=\"post__image\"><img loading=\"lazy\"  src=\"https://chandownbytheriver.github.io/media/posts/2/concdatadesc.PNG\" alt=\"\" width=\"1054\" height=\"647\" sizes=\"100vw\" srcset=\"https://chandownbytheriver.github.io/media/posts/2/responsive/concdatadesc-xs.PNG 300w ,https://chandownbytheriver.github.io/media/posts/2/responsive/concdatadesc-sm.PNG 480w ,https://chandownbytheriver.github.io/media/posts/2/responsive/concdatadesc-md.PNG 768w ,https://chandownbytheriver.github.io/media/posts/2/responsive/concdatadesc-lg.PNG 1024w ,https://chandownbytheriver.github.io/media/posts/2/responsive/concdatadesc-xl.PNG 1360w ,https://chandownbytheriver.github.io/media/posts/2/responsive/concdatadesc-2xl.PNG 1600w\"></figure>\n<p>The data seems to be rather evenly distributed around the mean, with two standard deviations bringing us close to our minimum and max. We can also see that none of the features contain empty cells. With no outliers or missing values this data is ready to be analyzed. </p>\n<p> </p>\n<figure class=\"post__image\"><img loading=\"lazy\"  src=\"https://chandownbytheriver.github.io/media/posts/2/traintestsplit-2.PNG\" alt=\"\" width=\"915\" height=\"110\" sizes=\"100vw\" srcset=\"https://chandownbytheriver.github.io/media/posts/2/responsive/traintestsplit-2-xs.PNG 300w ,https://chandownbytheriver.github.io/media/posts/2/responsive/traintestsplit-2-sm.PNG 480w ,https://chandownbytheriver.github.io/media/posts/2/responsive/traintestsplit-2-md.PNG 768w ,https://chandownbytheriver.github.io/media/posts/2/responsive/traintestsplit-2-lg.PNG 1024w ,https://chandownbytheriver.github.io/media/posts/2/responsive/traintestsplit-2-xl.PNG 1360w ,https://chandownbytheriver.github.io/media/posts/2/responsive/traintestsplit-2-2xl.PNG 1600w\"></figure>\n<p>We'll separate the data into the features that determine the output, and the targetted prediction column.</p>\n<figure class=\"post__image\"><img loading=\"lazy\"  src=\"https://chandownbytheriver.github.io/media/posts/2/regnnnotstandarized.PNG\" alt=\"\" width=\"944\" height=\"351\" sizes=\"100vw\" srcset=\"https://chandownbytheriver.github.io/media/posts/2/responsive/regnnnotstandarized-xs.PNG 300w ,https://chandownbytheriver.github.io/media/posts/2/responsive/regnnnotstandarized-sm.PNG 480w ,https://chandownbytheriver.github.io/media/posts/2/responsive/regnnnotstandarized-md.PNG 768w ,https://chandownbytheriver.github.io/media/posts/2/responsive/regnnnotstandarized-lg.PNG 1024w ,https://chandownbytheriver.github.io/media/posts/2/responsive/regnnnotstandarized-xl.PNG 1360w ,https://chandownbytheriver.github.io/media/posts/2/responsive/regnnnotstandarized-2xl.PNG 1600w\"></figure>\n<p>Next, we'll create a loop for the process of running the model. The data training and testing split is created, and then the model is initialized. The amount of input nodes is equal to the amount of features. There will be one hidden calculation layer between the input and the output. The Rectified Linear Unit (RELU) activation function will be used for higher efficiency and its ability to allow learning off of complex patterns. </p>\n<p>To start out, the Neural Network will go through 50 epochs. After each epoch, variable weights will be updated based on what the network learned in the last epoch. After each epoch, we will gather the mean squared error so that we see how accuracy progresses through training.</p>\n<p>By placing the train and test split as well as model generation in a loop we can get a sense of how the model will perform not just once, but however many times we'd like to see. This helps to ensure that our results aren't a fluke, but rather an accurate representation of how the model works. The model will not use the previous iteration's insights when making the next one. It's a fresh start each time.</p>\n<figure class=\"post__image\"><img loading=\"lazy\"  src=\"https://chandownbytheriver.github.io/media/posts/2/nonstandardresults.PNG\" alt=\"\" width=\"723\" height=\"249\" sizes=\"100vw\" srcset=\"https://chandownbytheriver.github.io/media/posts/2/responsive/nonstandardresults-xs.PNG 300w ,https://chandownbytheriver.github.io/media/posts/2/responsive/nonstandardresults-sm.PNG 480w ,https://chandownbytheriver.github.io/media/posts/2/responsive/nonstandardresults-md.PNG 768w ,https://chandownbytheriver.github.io/media/posts/2/responsive/nonstandardresults-lg.PNG 1024w ,https://chandownbytheriver.github.io/media/posts/2/responsive/nonstandardresults-xl.PNG 1360w ,https://chandownbytheriver.github.io/media/posts/2/responsive/nonstandardresults-2xl.PNG 1600w\"></figure>\n<p>Here we have our measures of accuracy gathered from the neural network's prediction performances. We can see from the Mean Squared Error and its standard deviation that this model produces answers at an average of just under 16 points away from the true label. We can also see that the model varies pretty widely from iteration to iteration. Is there anything we can do about that? </p>\n<p>Perhaps by normalizing the data we can produce more accurate and consistent results.</p>\n<figure class=\"post__image\"><img loading=\"lazy\"  src=\"https://chandownbytheriver.github.io/media/posts/2/normalize.PNG\" alt=\"\" width=\"891\" height=\"177\" sizes=\"100vw\" srcset=\"https://chandownbytheriver.github.io/media/posts/2/responsive/normalize-xs.PNG 300w ,https://chandownbytheriver.github.io/media/posts/2/responsive/normalize-sm.PNG 480w ,https://chandownbytheriver.github.io/media/posts/2/responsive/normalize-md.PNG 768w ,https://chandownbytheriver.github.io/media/posts/2/responsive/normalize-lg.PNG 1024w ,https://chandownbytheriver.github.io/media/posts/2/responsive/normalize-xl.PNG 1360w ,https://chandownbytheriver.github.io/media/posts/2/responsive/normalize-2xl.PNG 1600w\"></figure>\n<p>We will place the normalization into the loop and after the train/test split to ensure that the generated averages aren't affected by data that is hidden in the testing set.</p>\n<p>We'll proceed with the same model architechture otherwise, to see what effect this has.</p>\n<figure class=\"post__image\"><img loading=\"lazy\"  src=\"https://chandownbytheriver.github.io/media/posts/2/normalizeresults-2.PNG\" alt=\"\" width=\"507\" height=\"42\" sizes=\"100vw\" srcset=\"https://chandownbytheriver.github.io/media/posts/2/responsive/normalizeresults-2-xs.PNG 300w ,https://chandownbytheriver.github.io/media/posts/2/responsive/normalizeresults-2-sm.PNG 480w ,https://chandownbytheriver.github.io/media/posts/2/responsive/normalizeresults-2-md.PNG 768w ,https://chandownbytheriver.github.io/media/posts/2/responsive/normalizeresults-2-lg.PNG 1024w ,https://chandownbytheriver.github.io/media/posts/2/responsive/normalizeresults-2-xl.PNG 1360w ,https://chandownbytheriver.github.io/media/posts/2/responsive/normalizeresults-2-2xl.PNG 1600w\"></figure>\n<p>Interestingly, our error margin has increased, but the clustering has grown tighter. Perhaps by changing the structure of our neural network, we can reduce both.</p>\n<figure class=\"post__image\"><img loading=\"lazy\"  src=\"https://chandownbytheriver.github.io/media/posts/2/morehidden.PNG\" alt=\"\" width=\"487\" height=\"79\" sizes=\"100vw\" srcset=\"https://chandownbytheriver.github.io/media/posts/2/responsive/morehidden-xs.PNG 300w ,https://chandownbytheriver.github.io/media/posts/2/responsive/morehidden-sm.PNG 480w ,https://chandownbytheriver.github.io/media/posts/2/responsive/morehidden-md.PNG 768w ,https://chandownbytheriver.github.io/media/posts/2/responsive/morehidden-lg.PNG 1024w ,https://chandownbytheriver.github.io/media/posts/2/responsive/morehidden-xl.PNG 1360w ,https://chandownbytheriver.github.io/media/posts/2/responsive/morehidden-2xl.PNG 1600w\"></figure>\n<p>We'll see how adding two more hidden layers will affect the accuracy of predictions. Everything else will remain the same.</p>\n<figure class=\"post__image\"><img loading=\"lazy\"  src=\"https://chandownbytheriver.github.io/media/posts/2/mhidresults.PNG\" alt=\"\" width=\"512\" height=\"44\" sizes=\"100vw\" srcset=\"https://chandownbytheriver.github.io/media/posts/2/responsive/mhidresults-xs.PNG 300w ,https://chandownbytheriver.github.io/media/posts/2/responsive/mhidresults-sm.PNG 480w ,https://chandownbytheriver.github.io/media/posts/2/responsive/mhidresults-md.PNG 768w ,https://chandownbytheriver.github.io/media/posts/2/responsive/mhidresults-lg.PNG 1024w ,https://chandownbytheriver.github.io/media/posts/2/responsive/mhidresults-xl.PNG 1360w ,https://chandownbytheriver.github.io/media/posts/2/responsive/mhidresults-2xl.PNG 1600w\"></figure>\n<p>Allowing the neural network more steps to understand the connections between variables and outcomes predicatably reduced both MSE and the standard deviation of the MSE. </p>\n<p>The predictions are now, on average, within 9.76 (square root of 95.27) of the true label. This is within the realm of a prediction that could be relied upon. These changes have resulted in a 37.4% increase in accuracy compared to our first model.</p>",
            "author": {
                "name": "Chandler Rottenberg"
            },
            "tags": [
            ],
            "date_published": "2024-03-01T12:05:09-05:00",
            "date_modified": "2024-03-15T15:57:32-04:00"
        },
        {
            "id": "https://chandownbytheriver.github.io/simple-and-multiple-linear-regression-projects.html",
            "url": "https://chandownbytheriver.github.io/simple-and-multiple-linear-regression-projects.html",
            "title": "Improving Accuracy by Using Multiple Linear Regression",
            "summary": "&nbsp; &nbsp; &nbsp;A simple linear regression model uses one variable to attempt to predict an outcome. While this may offer enough accuracy to be considered right most of the time, real world problems will usually have multiple variables that affect what happens. Ignoring these extra&hellip;",
            "content_html": "\n  <p>\n    &nbsp; &nbsp; &nbsp;A simple linear regression model uses one variable to attempt to predict an outcome. While this may offer enough accuracy to be considered right most of the time, real world problems will usually have multiple variables that affect what happens. Ignoring these extra variables can cause problems if we want to rely on our model's predictions. How can we create a model that includes these variables, and does the extra work actually improve precision enough for it to be worth it?&nbsp;\n  </p>\n\n    <h2 id=\"determining-co2-emissions-based-on-vehicle-characteristics\">\n      Determining Co2 Emissions Based on Vehicle Characteristics\n    </h2>\n\n    <figure class=\"post__image post__image--center\">\n      <img loading=\"lazy\" src=\"https://chandownbytheriver.github.io/media/posts/1/datasample.PNG\" height=\"481\" width=\"1684\" alt=\"\"  sizes=\"100vw\" srcset=\"https://chandownbytheriver.github.io/media/posts/1/responsive/datasample-xs.PNG 300w ,https://chandownbytheriver.github.io/media/posts/1/responsive/datasample-sm.PNG 480w ,https://chandownbytheriver.github.io/media/posts/1/responsive/datasample-md.PNG 768w ,https://chandownbytheriver.github.io/media/posts/1/responsive/datasample-lg.PNG 1024w ,https://chandownbytheriver.github.io/media/posts/1/responsive/datasample-xl.PNG 1360w ,https://chandownbytheriver.github.io/media/posts/1/responsive/datasample-2xl.PNG 1600w\">\n      <figcaption>A look at the data being used for this model. 1,067 entries covering each model of car released in 2014.</figcaption>\n    </figure>\n\n    <figure class=\"post__image post__image--center\">\n      <img loading=\"lazy\" src=\"https://chandownbytheriver.github.io/media/posts/1/occurencehist.PNG\" height=\"578\" width=\"623\" alt=\"\"  sizes=\"100vw\" srcset=\"https://chandownbytheriver.github.io/media/posts/1/responsive/occurencehist-xs.PNG 300w ,https://chandownbytheriver.github.io/media/posts/1/responsive/occurencehist-sm.PNG 480w ,https://chandownbytheriver.github.io/media/posts/1/responsive/occurencehist-md.PNG 768w ,https://chandownbytheriver.github.io/media/posts/1/responsive/occurencehist-lg.PNG 1024w ,https://chandownbytheriver.github.io/media/posts/1/responsive/occurencehist-xl.PNG 1360w ,https://chandownbytheriver.github.io/media/posts/1/responsive/occurencehist-2xl.PNG 1600w\">\n      <figcaption>Histograms displaying the occurrences of variables that affect the emissions output. We can see that each variable follows a similar bell curve shape. How can we determine which variable to use when building a simple model?</figcaption>\n    </figure>\n\n  <div  class=\"gallery-wrapper\">\n    <div class=\"gallery\" data-columns=\"3\">\n      <figure class=\"gallery__item\">\n      <a href=\"https://chandownbytheriver.github.io/media/posts/1/gallery/cylindersvsemissionslinearity.PNG\" data-size=\"610x580\">\n        <img loading=\"lazy\" src=\"https://chandownbytheriver.github.io/media/posts/1/gallery/cylindersvsemissionslinearity-thumbnail.PNG\" height=\"580\" width=\"610\" alt=\"Linearity between number of cylinders and emissons\" >\n      </a>\n      \n    </figure><figure class=\"gallery__item\">\n      <a href=\"https://chandownbytheriver.github.io/media/posts/1/gallery/enginesizevsemissionslinearity-2.PNG\" data-size=\"611x580\">\n        <img loading=\"lazy\" src=\"https://chandownbytheriver.github.io/media/posts/1/gallery/enginesizevsemissionslinearity-2-thumbnail.PNG\" height=\"580\" width=\"611\" alt=\"Linearity between engine size and emissions\" >\n      </a>\n      \n    </figure><figure class=\"gallery__item\">\n      <a href=\"https://chandownbytheriver.github.io/media/posts/1/gallery/fuelconsvsemissionslinearity.PNG\" data-size=\"607x588\">\n        <img loading=\"lazy\" src=\"https://chandownbytheriver.github.io/media/posts/1/gallery/fuelconsvsemissionslinearity-thumbnail.PNG\" height=\"588\" width=\"607\" alt=\"Linearity between combined fuel consumption and emissions\" >\n      </a>\n      \n    </figure>\n    </div>\n  </div>\n\n  <p class=\"align-center\">\n    Linearity of the relationships between the targeted variables and C02 emissions are displayed. The linearity of their relations, or how strongly they are correlated, can be used as an indicator of what may be the best single predictor. Let's see how using Engine Size turns out.\n  </p>\n\n    <figure class=\"post__image post__image--center\">\n      <img loading=\"lazy\" src=\"https://chandownbytheriver.github.io/media/posts/1/simplelinearmodelcreation.PNG\" height=\"865\" width=\"608\" alt=\"\"  sizes=\"100vw\" srcset=\"https://chandownbytheriver.github.io/media/posts/1/responsive/simplelinearmodelcreation-xs.PNG 300w ,https://chandownbytheriver.github.io/media/posts/1/responsive/simplelinearmodelcreation-sm.PNG 480w ,https://chandownbytheriver.github.io/media/posts/1/responsive/simplelinearmodelcreation-md.PNG 768w ,https://chandownbytheriver.github.io/media/posts/1/responsive/simplelinearmodelcreation-lg.PNG 1024w ,https://chandownbytheriver.github.io/media/posts/1/responsive/simplelinearmodelcreation-xl.PNG 1360w ,https://chandownbytheriver.github.io/media/posts/1/responsive/simplelinearmodelcreation-2xl.PNG 1600w\">\n      <figcaption>The creation and training of a simple linear regression model using engine size to predict emissions. The model will use the visualized line of best fit to predict emissions</figcaption>\n    </figure>\n\n    <figure class=\"post__image post__image--center\">\n      <img loading=\"lazy\" src=\"https://chandownbytheriver.github.io/media/posts/1/simpresults1.PNG\" height=\"287\" width=\"656\" alt=\"\"  sizes=\"100vw\" srcset=\"https://chandownbytheriver.github.io/media/posts/1/responsive/simpresults1-xs.PNG 300w ,https://chandownbytheriver.github.io/media/posts/1/responsive/simpresults1-sm.PNG 480w ,https://chandownbytheriver.github.io/media/posts/1/responsive/simpresults1-md.PNG 768w ,https://chandownbytheriver.github.io/media/posts/1/responsive/simpresults1-lg.PNG 1024w ,https://chandownbytheriver.github.io/media/posts/1/responsive/simpresults1-xl.PNG 1360w ,https://chandownbytheriver.github.io/media/posts/1/responsive/simpresults1-2xl.PNG 1600w\">\n      <figcaption>Here are the testing results for the model using engine size. The MSE shows that on average, predictions deviate from the actual observations by the square root of 845.64 (29.08). The r^2 score indicates that 79% of the observations are correctly explained using engine size as the predictor. This is okay, but we can do better.</figcaption>\n    </figure>\n\n    <figure class=\"post__image post__image--center\">\n      <img loading=\"lazy\" src=\"https://chandownbytheriver.github.io/media/posts/1/simpresults2.PNG\" height=\"597\" width=\"689\" alt=\"\"  sizes=\"100vw\" srcset=\"https://chandownbytheriver.github.io/media/posts/1/responsive/simpresults2-xs.PNG 300w ,https://chandownbytheriver.github.io/media/posts/1/responsive/simpresults2-sm.PNG 480w ,https://chandownbytheriver.github.io/media/posts/1/responsive/simpresults2-md.PNG 768w ,https://chandownbytheriver.github.io/media/posts/1/responsive/simpresults2-lg.PNG 1024w ,https://chandownbytheriver.github.io/media/posts/1/responsive/simpresults2-xl.PNG 1360w ,https://chandownbytheriver.github.io/media/posts/1/responsive/simpresults2-2xl.PNG 1600w\">\n      <figcaption>By retraining the model to predict based on the combined fuel consumption variable, you can see that we are able to reduce the error across the board. However, it is still not as accurate as we may want in a predictive model.</figcaption>\n    </figure>\n\n  <p>\n    As shown by the linearity graphs earlier, there are multiple variables that have a high correlation to the outcome of the emissions. Perhaps by training the model to use all three variables, we can achieve better results.\n  </p>\n\n    <figure class=\"post__image post__image--center\">\n      <img loading=\"lazy\" src=\"https://chandownbytheriver.github.io/media/posts/1/multicoefficients.PNG\" height=\"214\" width=\"643\" alt=\"\"  sizes=\"100vw\" srcset=\"https://chandownbytheriver.github.io/media/posts/1/responsive/multicoefficients-xs.PNG 300w ,https://chandownbytheriver.github.io/media/posts/1/responsive/multicoefficients-sm.PNG 480w ,https://chandownbytheriver.github.io/media/posts/1/responsive/multicoefficients-md.PNG 768w ,https://chandownbytheriver.github.io/media/posts/1/responsive/multicoefficients-lg.PNG 1024w ,https://chandownbytheriver.github.io/media/posts/1/responsive/multicoefficients-xl.PNG 1360w ,https://chandownbytheriver.github.io/media/posts/1/responsive/multicoefficients-2xl.PNG 1600w\">\n      <figcaption>After fitting the model to the three relevant variables, we are given the coefficients for each variable. In other words, how relevant to the prediction each feature is. </figcaption>\n    </figure>\n\n    <figure class=\"post__image post__image--center\">\n      <img loading=\"lazy\" src=\"https://chandownbytheriver.github.io/media/posts/1/multipredict.PNG\" height=\"349\" width=\"647\" alt=\"\"  sizes=\"100vw\" srcset=\"https://chandownbytheriver.github.io/media/posts/1/responsive/multipredict-xs.PNG 300w ,https://chandownbytheriver.github.io/media/posts/1/responsive/multipredict-sm.PNG 480w ,https://chandownbytheriver.github.io/media/posts/1/responsive/multipredict-md.PNG 768w ,https://chandownbytheriver.github.io/media/posts/1/responsive/multipredict-lg.PNG 1024w ,https://chandownbytheriver.github.io/media/posts/1/responsive/multipredict-xl.PNG 1360w ,https://chandownbytheriver.github.io/media/posts/1/responsive/multipredict-2xl.PNG 1600w\">\n      <figcaption>Now, the model will use these coefficients to make predictions. As we can see, the accuracy metrics have all improved. The MSE has been reduced by 37% from our first attempt, and the variance score has increased from at first .79, then to .82, and now finally to .87.</figcaption>\n    </figure>\n\n  <p>\n    The 37% reduction in the average size of error (MSE) and an 8 point increase in the prediction explaining capability of the model are certainly worth the extra lines of code in this case, and in most cases.\n  </p>",
            "author": {
                "name": "Chandler Rottenberg"
            },
            "tags": [
            ],
            "date_published": "2024-02-25T20:21:54-05:00",
            "date_modified": "2024-03-01T12:23:31-05:00"
        }
    ]
}
